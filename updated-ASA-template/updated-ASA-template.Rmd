---
title: "Perception and Cognitive Implications of Logarithmic Scales for Exponentially Increasing Data: Perceptual Sensitivity Tested with Statistical Lineups"

# to produce blinded version set to 1
blinded: 0

authors: 
- name: Emily A. Robinson 1
  affiliation: Department of Statistics, California Polytechnic State University - San Luis Obispo
  
- name: Reka Howard 2
  affiliation: Department of Statistics, University of Nebraska - Lincoln
  
- name: Susan VanderPlas 3
  affiliation: Department of Statistics, University of Nebraska - Lincoln

keywords:
- log scales
- visual inference
- graphical testing

abstract: |
  Logarithmic transformations are a standard solution to displaying data that span several magnitudes within a single graph. This paper investigates the impact of log scales on perceptual sensitivity through a visual inference experiment using statistical lineups. Our study evaluated participant's ability to detect differences between exponentially increasing data, characterized by varying levels of curvature, using both linear and logarithmic scales. Participants were presented with a series of plots and asked to identify the panel that appeared most different from the others. Due to the choice of scale altering the contextual appearance of the data, the results revealed slight perceptual advantages for both scales depending on the curvatures of the compared data. This study serves as the initial part of a three-paper series dedicated to understanding the perceptual and cognitive implications of using logarithmic scales for visualizing exponentially increasing data. These studies serve as an example of multi-modal graphical testing, examining different levels of engagement and interaction with graphics to establish nuanced and specific guidelines for graphical design.

bibliography: bibliography.bib
biblio-style: apalike # default is using plain.bst
output: rticles::asa_article

header-includes:
- \usepackage[dvipsnames]{xcolor} % colors
- \newcommand{\ear}[1]{{\textcolor{blue}{#1}}}
- \newcommand{\svp}[1]{{\textcolor{RedOrange}{#1}}}
- \newcommand{\rh}[1]{{\textcolor{Green}{#1}}}
- \newcommand\pcref[1]{(\cref{#1})}
- \usepackage{algorithm,algpseudocode,booktabs}
- \usepackage{hyperref}
- \usepackage[capitalise]{cleveref}
# - \usepackage{xr} \externaldocument{appendix}
# - \usepackage[authoryear]{natbib}

---

```{r setup, include = F}
options(width = 60)
knitr::opts_chunk$set(
  echo = F, 
  eval = T, 
  message = F, 
  warning = F,
  fig.width = 6, 
  fig.height = 4,  
  fig.align = 'center',
  out.width = "\\linewidth", 
  dpi = 300, 
  tidy = T, tidy.opts=list(width.cutoff=45),
  fig.pos = "tbp",
  out.extra = "",
  cache = FALSE
)

library(readr)
library(tidyverse)
library(ggtext)
library(scales)
library(knitr)
library(gridExtra)
library(patchwork)
library(cowplot)
# library(ggforce)
# library(formatR)
```

# Introduction

Effective communication of data is critical in influencing people's opinions and actions. 
This consideration was particularly true during the COVID-19 pandemic, where data visualizations and dashboards were vital in informing the public and policymakers about the outbreak's status.
Local governments relied on graphics to inform their decisions about shutdowns and mask mandates, while residents were presented with data visualizations to encourage compliance with these regulations.
A major issue designers encountered when creating COVID-19 plots was how to display data from a wide range of values [@fagen-ulmschneider_2020, @burnmurdoch_2020].
When faced with data that span several orders of magnitude, we must decide whether to show the data on its original scale (compressing the smaller magnitudes into a relatively small area) or to transform the scale and alter the contextual appearance of the data. 
Log axis transformations have emerged as a standard solution to this challenge, as they allow for the display of data over several orders of magnitude within a single graph.

Exponential data is one such example of a function that compresses smaller magnitudes into a smaller area; \cref{fig:log-scales} presents hard drive capacity over the past forty years on both the linear and log scales to demonstrate the usefulness of log scales when dealing with data spanning multiple magnitudes.
Logarithms facilitate the conversion of multiplicative relationships (displaying 1 & 10 with a distance of 10 units apart and displaying 10 & 100 with a distance of 90 units apart) to additive relationships (displaying 1 & 10 and 10 & 100 an equal distance apart), highlighting proportional relationships and linearizing power functions [@menge_logarithmic_2018]. 
Logarithms also have practical applications, simplifying the computation of small numbers such as likelihoods and transforming data to conform to statistical assumptions.
Although log scales have a long history of use in fields such as ecology, psychophysics, engineering, and physics [@heckler_student_2013; @waddell2005comparisons], there is still a need to understand the implications of their use and provide best practices for their implementation.

```{r log-scales, fig.cap = "These plots present hard drive capacity over the past forty years on both the linear and log scale and illustrate the use of the log scale when displaying data which spans several magnitudes.", out.width="100%"}

# data <- tibble(x = seq(0,10, by = 0.1), y = exp(x))

mem_data <- read_csv("data/computer-memory-data.csv")

linear_scale <- mem_data |>
  filter(type == "Hard Drive Capacity") |> 
  ggplot(aes(x = dec_date, y = value)) +
  geom_point(shape = 1, alpha = 0.6) +
  theme_bw() +
  theme(aspect.ratio = 1) +
  ggtitle("Linear Scale") +
  labs(x = "Year",
       y = "",
       subtitle = "Hard Drive Capacity")

log_scale <- mem_data |>
  filter(type == "Hard Drive Capacity") |> 
  ggplot(aes(x = dec_date, y = value)) +
  geom_point(shape = 1, alpha = 0.6) +
  theme_bw() +
  theme(aspect.ratio = 1) +
  scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
                labels = scales::comma) +
  ggtitle("Log Scale") +
  labs(x = "Year",
       y = "",
       subtitle = "Hard Drive Capacity")

linear_scale + log_scale
```

Apart from the biases resulting from using log scales, there is a general misinterpretation of exponential growth. Early stages of exponential growth often appear to have a small growth rate, while the middle stage seems to exhibit more quadratic growth. It is only in the later stages that the exponential growth becomes apparent. \cref{fig:exponential-stages} highlights the three stages and associated appearances of exponential growth at each stage [@vonbergmann_2021]. 
This misinterpretation can lead to decisions made under inaccurate understanding, resulting in potential consequences.

```{r exponential-stages, fig.cap = "This figure highlights the three stages and associated appearances of exponential growth at each stage. Early stages of exponential growth often appear to have a small growth rate, while the middle stage seems to exhibit more quadratic growth. It is only in the later stages that the exponential growth becomes apparent.", out.width="100%"}
knitr::include_graphics("images/exponential-stages-comic.jpg")
```

```{r exponential-stages-v2, include = FALSE, fig.cap = "Highlights the three stages and associated appearances of exponential growth at each stage. Early stages of exponential growth often appear to have a small growth rate, while the middle stage seems to exhibit more quadratic growth. It is only in the later stages that the exponential growth becomes apparent.", out.width="100%"}
library(ggforce)

data <- tibble(x = seq(0,6,0.001),
               y = exp(x)) %>%
  mutate(group = ifelse(x <= 2, "G1", 
                        ifelse(x <= 4, "G2", "G3")))

data |> 
  ggplot(aes(x = x, y = y, color = group)) +
  geom_line(show.legend = T, linewidth = 1.2) +
  theme_bw() +
  theme(aspect.ratio = 1) +
  # facet_zoom(xlim = c(0,2), shrink = F, zoom.size = 3) +
  # facet_zoom(xy = group == 'G1', zoom.size = 1/3) +
  geom_mark_rect(show.legend = T) +
  geom_text(x = 0.8, y = 50, label = "Early", color = "#1B9E77", size = 4) +
  geom_text(x = 2.8, y = 100, label = "Middle", color = "#D95F02", size = 4) +
  geom_text(x = 4.8, y = 300, label = "Late", color = "#7570B3", size = 4) +
  scale_color_brewer("Stage", palette = "Dark2", labels = c("Early", "Middle", "Late")) +
  labs(x = "",
       y = "",
       title = "Stages of Exponential Growth") +
  theme(axis.text = element_blank(),
        legend.position = "none"
        )
```

Previous studies have explored the estimation and prediction of exponential growth and found that individuals often underestimate exponential growth when presented values numerically and graphically [@wagenaar_misperception_1975]. The hierarchy of plot objects, such as lengths and angles, as described by @cleveland_graphical_1985, offers a possible explanation for the underestimation observed in exponentially increasing trends. Experiments conducted by @wagenaar_misperception_1975, @jones_polynomial_1977, and @mackinnon_feedback_1991 aimed to improve estimation accuracy for exponential growth. While contextual knowledge or experience did not enhance estimation, instruction on exponential growth reduced underestimation by prompting participants to adjust their initial starting value [@wagenaar_misperception_1975; @jones_polynomial_1977]. Furthermore, providing immediate feedback to participants about the accuracy of their predictions improved estimation [@mackinnon_feedback_1991].

Log transforming the data may address our inability to predict exponential growth accurately. However, this transformation introduces new complexities, as most readers may need to be mathematically sophisticated enough to intuitively understand logarithmic math and translate it back into real-world effects.
Despite the transformative power of logarithmic scales in facilitating accurate data representation, @menge_logarithmic_2018's survey of ecologists highlights the challenges associated with the widespread comprehension of log-scaled data. Notably, the study identifies prevalent misconceptions arising from linear extrapolation assumptions in log-log space, a factor that often leads to neglect of the underlying exponential relationships in linear-linear space.

Building upon the need for a nuanced understanding of data representation, @buja_statistical_2009 introduced statistical lineups as a framework for statistical inference and graphical tests.
Statistical lineups treat a data plot as a visual statistic, summarizing the data as a numerical function or mapping.
Evaluation of a panel in a statistical lineup requires visual inspection by a person, and if visual evaluations lead to different results, two visualization methods are deemed significantly different.
Recent studies have utilized statistical lineups to quantify the perception of graphical design choices [@hofmann_graphical_2012; @loy_model_2017; @loy_variations_2016; @vanderplas_clusters_2017]. 
Statistical lineups provide an elegant way of combining perception and statistical hypothesis testing through graphical experiments [@majumder_validation_2013; @vanderplas_testing_2020; @wickham2010graphical].

The term 'lineup' is an analogy to police lineups in criminal investigations, where witnesses identify the criminal from a group of individuals. 
Similarly, researchers present a statistical lineup plot consisting of smaller panels and ask the viewer to identify the panel that contains the actual data from a set of decoy null plots. 
Researchers generate null plots containing data generated according to a prespecified hypothesis using permutation or simulation.
Typically, a statistical lineup consists of 20 panels, with one target panel and 19 null panels.
If the viewer can identify the target panel from the null panels, it suggests that the actual data is visually distinct from the data generated under the null model. 

While explicit graphical tests direct the participant to a specific feature of a plot to answer a particular question, implicit graphical tests require the user to identify both the purpose and function of the plot in order to evaluate the plots shown. 
Furthermore, implicit graphical tests, such as lineups, simultaneously test for multiple visual features, including outliers, clusters, and linear and nonlinear relationships [@vanderplas2015spatial].
Researchers can collect responses from multiple viewers using crowd-sourcing websites such as Prolific and Amazon Mechanical Turk.

In this paper, our primary focus is to evaluate the benefits and drawbacks of using log scales, specifically delving into their impact on perceptual sensitivity towards the degree of curvature. To address this, we conducted a visual inference experiment employing statistical lineups [@buja_statistical_2009]. Although our findings could have broad applications to various functions resulting in curvature, our experiment deliberately centered on participants' ability to identify differences in the curvature of exponentially increasing curves when presented with both linear and log scales. We discuss the nuances and challenges of testing the perception of exponential growth in the appendix. Importantly, this investigation did not necessitate participants to undergo mathematical training or possess a prior understanding of exponential growth or logarithmic scales. Instead, it aimed to unravel the inherent ability to identify differences in curvature within charts, focusing on the fundamental nature of visual perception.

In [Section 2](#methods) we describe the participant sample, the graphical task, data generation process, and study design.
[Section 3](#results) describes the participant data collected and shares results from the statistical analyses of the data using a generalized linear mixed model.
We present overall conclusions and discussion of the results in [Section 4](#conclusion-discussion), and provide an overview of future related papers.
The [Supplementary Material](#supplementary-material) includes a link to the RShiny data collection applet, participant data used for analysis, and code to replicate the analysis.
The results of this study lay the groundwork for further exploration of the implications of using log scales in data visualization.

# Study Development and Methods {#methods}

## Data Generation

In this study, we simulated data from an exponential model to generate the target and null data sets; the models between panels differ in the parameter values selected for the null and target panels.
In order to guarantee the simulated data spans the same domain and range of values for each statistical lineup panel, we began with a domain constraint of $x\in [0,20]$ and a range constraint of $y\in [10,100]$ with $N = 50$ points randomly assigned throughout the domain. We mapped the randomly generated $x$ values to a corresponding $y$ value based on an exponential model with predetermined parameter values and multiplicative random errors to simulate the response.
These constraints assure that participants who select the target panel are doing so because of their visual perception differentiating between curvature or growth rate rather than different starting or ending values.

We simulated data based on a three-parameter exponential model with multiplicative errors: 
\begin{align}
y_i & = \alpha\cdot e^{\beta\cdot x_i + \epsilon_i} + \theta \\
\text{with } \epsilon_i & \sim N(0, \sigma^2). \nonumber
\end{align} 
The parameters $\alpha$ and $\theta$ were adjusted based on $\beta$ and $\sigma^2$ to guarantee the range and domain constraints are met. 
The model generated $N = 50$ points $(x_i, y_i), i = 1,...,N$ where $x$ and $y$ have an increasing exponential relationship. 
The heuristic data generation procedure is described in \cref{alg:lineup-parameter-estimation-algorithm} and \cref{alg:lineup-exponential-data-simulation-algorithm}.

\begin{algorithm}
  \caption{Lineup Parameter Estimation}\label{alg:lineup-parameter-estimation-algorithm}
  \begin{algorithmic}[1]
    \Statex \hspace*{-1em}\textbullet~\textbf{Input Parameters:} domain $x\in[0,20]$, range $y\in[10,100]$, midpoint $x_{mid}$.
    \Statex \hspace*{-1em}\textbullet~\textbf{Output Parameters:} estimated model parameters $\hat\alpha, \hat\beta, \hat\theta$.
    \State In order to obtain the two middle points (total of four points for estimating three parameters), determine the $y=-x$ line scaled to fit the assigned domain and range.
    \State Map the values $x_{mid} - 0.1$ and $x_{mid} + 0.1$ to the $y=-x$ line for the two additional points.
    \State From the set of points $(x_k, y_k)$ for $k = 1,2,3,4$, calculate the coefficients from the linear regression model $\ln(y_k) = b_0 +b_1x_k$ to obtain starting values for $\alpha_0 = e^{b_0}, \beta_0 =  b_1, \theta_0 = 0.5\cdot \min(y)$
    \State Using the \texttt{nls} function from the base \texttt{stats} package in Rstudio [@Rstudio] and the starting parameter values - $\alpha_0, \beta_0, \theta_0$ - fit the nonlinear model, $y_k = \alpha\cdot e^{\beta\cdot x_k}+\theta$ to get estimated parameter values for $\hat\alpha, \hat\beta, \hat\theta.$
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}
  \caption{Lineup Exponential Data Simulation}\label{alg:lineup-exponential-data-simulation-algorithm}
  \begin{algorithmic}[1]
    \Statex \hspace*{-1em}\textbullet~\textbf{Input Parameters:} sample size $N = 50$, estimated parameters $\hat\alpha$, $\hat\beta$, and $\hat\theta$, from \cref{alg:lineup-parameter-estimation-algorithm}, and standard deviation $\sigma$ from the exponential curve.
    \Statex \hspace*{-1em}\textbullet~\textbf{Output Parameters:} $N$ points, in the form of vectors $\mathbf{x}$ and $\mathbf{y}$.
    \State Generate $\tilde x_j, j = 1,..., \frac{3}{4}N$ as a sequence of evenly spaced points in $[0,20]$. This ensures the full domain of $x$ is used, fulfilling the constraints of spanning the same domain and range for each parameter combination.
    \State Obtain $\tilde x_i, i = 1,...,N$ by sampling $N = 50$ values from the set of $\tilde x_j$ values. This guarantees some variability and potential clustering in the exponential growth curve disrupting the perception due to continuity of points.
    \State Obtain the final $x_i$ values by jittering $\tilde x_i$.
    \State Calculate $\tilde\alpha = \frac{\hat\alpha}{e^{\sigma^2/2}}.$ This ensures that the range of simulated values for different standard deviation parameters has an equal expected value for a given rate of change due to the non-constant variance across the domain.
    \State Generate $y_i = \tilde\alpha\cdot e^{\hat\beta x_i + e_i}+\hat\theta$ where $e_i\sim N(0,\sigma^2).$
  \end{algorithmic}
\end{algorithm}

## Parameter Selection {#lineups-parameter-selection}

We chose three levels of trend curvature (low curvature, medium curvature, and high curvature). For each curvature level, we simulated 1,000 data sets of $(x_{ij}, y_{ij})$ points for $i = 1,...,50$ increments of $x$-values and replicated $j = 1,...,10$ corresponding $y$-values per $x$-value.
Each generated $x_i$ point from \cref{alg:lineup-exponential-data-simulation-algorithm} was replicated ten times. 
We fit a linear regression model on each of the individual data sets and computed the lack of fit statistic (LOF) which measures the deviation of the data from the linear regression model.
After obtaining the LOF statistic for each level of curvature, we evaluated the density plots \pcref{fig:lof-density-curves} to provide a metric for differentiating between the curvature levels and thus detecting the target plot.
While the LOF statistic provides a numerical value for discriminating between the difficulty levels, it cannot be directly related to the perceptual discriminability; it serves primarily as an approximation to ensure that we are testing parameters at several distinct curvature levels.
\cref{tab:parameter-data} lists the final parameters used for data simulation.

```{r lof-density-curves, fig.height = 4, fig.width = 8, fig.cap = "Density plot of the lack of fit statistic showing separation of difficulty levels: obvious curvature, noticable curvature, and almost linear.", out.width = "100%"}
lofData <- read.csv(file = "data/lineup-lof-data.csv")

lofPlot_curvature <- lofData |>
  mutate(Curvature = factor(Curvature, 
                            levels = c("Obvious Curvature", 
                                       "Noticeable Curvature", 
                                       "Almost Linear"
                                       ),
                            labels = c("High Curvature", 
                                      "Medium Curvature", 
                                      "Low Curvature")
                            ),
         Variability = factor(Variability, levels = c("Low"))
         ) |>
  ggplot(aes(x = statistic, 
             fill = Curvature)
         ) +
  geom_density(alpha = 0.6, aes(color = Curvature)) +
  scale_fill_manual("Difficulty", values = c("#004400", "#116611", "#55aa55")) +
  scale_color_manual("Difficulty", values = c("#004400", "#116611", "#55aa55")) +
  theme_bw(base_size = 14) +
  theme(legend.position = "none",
        axis.text.x  = element_text(size = 12),
        axis.text.y  = element_blank(),
        axis.ticks.y = element_blank(),
        plot.title   = element_markdown(size = 14,
                                    lineheight = 1.2),
        panel.grid.minor = element_blank()
        ) +
  scale_x_continuous("", limits = c(0,800), breaks = seq(0,800,100), expand = c(0.01,0.01)) +
  scale_y_continuous("", limits = c(0, 0.025)) +
  labs(title = "Comparison of Densities for the Lack of Fit Statistic <br> between <span style = 'color:#004400'>**High**</span>, <span style = 'color:#116611'>**Medium**</span>, and <span style = 'color:#55aa55'>**Low**</span> Curvature Levels"
        )
lofPlot_curvature
```

```{r parameter-data}
parameter_data <- read.csv(file = "data/lineup-parameter-data.csv")
parameter_data |>
  mutate(difficulty = case_when(difficulty == "Obvious Curvature" ~ "High",
                                difficulty == "Noticable Curvature" ~ "Medium",
                                difficulty == "Almost Linear" ~ "Low"
                                )
         ) |>
  select(difficulty, xMid, alphahat, alphatilde, betahat, thetahat, sigma_vals) |>
  knitr::kable("latex", 
               digits = 2, 
               escape = F, 
               booktabs = T, 
               linesep = "", 
               align='lrrrrrr',
               label = "parameter-data",
               col.names = c("Curvature Level",   
                             "$x_{mid}$", 
                             "$\\hat\\alpha$", 
                             "$\\tilde\\alpha$", 
                             "$\\hat\\beta$",
                             "$\\hat\\theta$", 
                             "$\\hat\\sigma$"
                             ),
        caption = "Lineup data simulation final parameters"
        )
```

## Lineup Setup 

To generate the small multiple scatter plots for the statistical lineups shown to participants in the study, we simulated a single data set corresponding to curvature level A for the target plot and multiple data sets corresponding to curvature level B for the null plots.
The `nullabor` package in R [@buja_statistical_2009] randomly assigned the target plot to one of the panels surrounded by panels containing null plots.
<!-- For example, the statistical lineup randomly embeds a target plot with simulated data following an increasing exponential curve with high curvature within null plots with simulated data following an increasing exponential trend with low curvature.  -->
The target and null panels span a similar domain and range due to the implemented constraints when simulating the data; the rationale for this decision is based on preattentive feature 
perception [@wolfe2019preattentive] and is discussed in detail in the appendix.

There were a total of six lineup curvature combinations; \cref{fig:curvature-combination-example} illustrates the six lineup curvature combinations (top: linear scale; bottom: log scale) where the solid line indicates the curvature level designated to the target plot while the dashed line indicates the curvature level assigned to the null plots.
Two sets of each lineup curvature combination were simulated (a total of twelve test data sets) and plotted on both the linear scale and the log scale (24 test lineup plots).
In addition, three curvature combinations generated homogeneous "Rorschach" lineups, where all panels were from the same distribution.
Each participant evaluated one "Rorschach" lineup. Results from the "Rorschach" evaluations indicate null panel selections were distributed relatively evenly with multiple candidates for the most interesting panel. We display and further discuss the "Rorschach" evaluation results in the appendix.

```{r curvature-combination-example, fig.height = 6, fig.width = 9, fig.cap = "Thumbnail plots illustrating the six curvature combinations displayed on both scales (linear and log). The solid line indicates the curvature level to be identified as the target plot from amongst a set of null plots with the curvature level indicated by the dashed line.", out.width="100%"}
simData <- read.csv("data/difficulty-comparison-data.csv")

# simData  |>
#   mutate(curvature = factor(curvature, levels = c("E", "M", "H"), labels = c("High Curvature", "Medium Curvature", "Low Curvature"))) |>
#   ggplot(aes(x = x, y = y, color = curvature)) +
#   geom_line(size = 1) +
#   theme_bw() +
#   theme(aspect.ratio = 1) +
#   theme(axis.text = element_blank(),
#         axis.title = element_blank(),
#         legend.position = "bottom") +
#   scale_color_manual("Difficulty", values = c("#004400", "#116611", "#55aa55"))

tE_nM <- simData  |>
  mutate(Target = "High",
         Null = "Medium") |>
  filter(curvature %in% c("E", "M")) |>
  mutate(curvature = factor(curvature, levels = c("E", "M", "H"))) |>
  ggplot(aes(x = x, y = y, color = curvature, linetype = curvature)) +
  geom_line(linewidth = 1) +
  theme_test() +
  theme(aspect.ratio = 1) +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        legend.position = "none") +
  scale_x_continuous(limits = c(0,20)) +
  scale_color_manual(values = c("green3", "black")) +
  scale_linetype_manual(values = c("solid", "dashed"))

tE_nH <- simData  |>
  mutate(Target = "High",
         Null = "Low") |>
  filter(curvature %in% c("E", "H")) |>
  mutate(curvature = factor(curvature, levels = c("E", "M", "H"))) |>
  ggplot(aes(x = x, y = y, color = curvature, linetype = curvature)) +
  geom_line(linewidth = 1) +
  theme_test() +
  theme(aspect.ratio = 1) +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        legend.position = "none") +
  scale_x_continuous(limits = c(0,20)) +
  scale_color_manual(values = c("green3", "black")) +
  scale_linetype_manual(values = c("solid", "dashed"))

tM_nE <- simData  |>
  mutate(Target = "Medium",
         Null = "High") |>
  filter(curvature %in% c("E", "M")) |>
  mutate(curvature = factor(curvature, levels = c("E", "M", "H"))) |>
  ggplot(aes(x = x, y = y, color = curvature, linetype = curvature)) +
  geom_line(linewidth = 1) +
  theme_test() +
  theme(aspect.ratio = 1) +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        legend.position = "none") +
  scale_x_continuous(limits = c(0,20)) +
  scale_color_manual(values = c("black", "green3")) +
  scale_linetype_manual(values = c("dashed", "solid"))

tM_nH <- simData  |>
  mutate(Target = "Medium",
         Null = "Low") |>
  filter(curvature %in% c("H", "M")) |>
  mutate(curvature = factor(curvature, levels = c("E", "M", "H"))) |>
  ggplot(aes(x = x, y = y, color = curvature, linetype = curvature)) +
  geom_line(linewidth = 1) +
  theme_test() +
  theme(aspect.ratio = 1) +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        legend.position = "none") +
  scale_x_continuous(limits = c(0,20)) +
  scale_color_manual(values = c("green3", "black")) +
  scale_linetype_manual(values = c("solid", "dashed"))

tH_nE <- simData  |>
  mutate(Target = "Low",
         Null = "High") |>
  filter(curvature %in% c("E", "H")) |>
  mutate(curvature = factor(curvature, levels = c("E", "M", "H"))) |>
  ggplot(aes(x = x, y = y, color = curvature, linetype = curvature)) +
  geom_line(linewidth = 1) +
  theme_test() +
  theme(aspect.ratio = 1) +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        legend.position = "none") +
  scale_x_continuous(limits = c(0,20)) +
  scale_color_manual(values = c("black", "green3")) +
  scale_linetype_manual(values = c("dashed", "solid"))

tH_nM <- simData  |>
  mutate(Target = "Low",
         Null = "Medium") |>
  filter(curvature %in% c("H", "M")) |>
  mutate(curvature = factor(curvature, levels = c("E", "M", "H"))) |>
  ggplot(aes(x = x, y = y, color = curvature, linetype = curvature)) +
  geom_line(linewidth = 1) +
  theme_test() +
  theme(aspect.ratio = 1) +
  theme(axis.text = element_blank(),
        axis.title = element_blank(),
        legend.position = "none") +
  scale_x_continuous(limits = c(0,20)) +
  scale_color_manual(values = c("black", "green3")) +
  scale_linetype_manual(values = c("dashed", "solid"))

# linear scale

tE_nH_linear <- tE_nH +
  scale_y_continuous(limits = c(10,100))

tE_nM_linear <- tE_nM +
  scale_y_continuous(limits = c(10,100))

tM_nH_linear <- tM_nH +
  scale_y_continuous(limits = c(10,100))

tM_nE_linear <- tM_nE +
  scale_y_continuous(limits = c(10,100))

tH_nM_linear <- tH_nM +
  scale_y_continuous(limits = c(10,100))

tH_nE_linear <- tH_nE +
  scale_y_continuous(limits = c(10,100))

# log scale

library(scales)

tE_nH_log <- tE_nH +
  scale_y_continuous(limits = c(10,100), trans = "log10")

tE_nM_log <- tE_nM +
  scale_y_continuous(limits = c(10,100), trans = "log10")

tM_nH_log <- tM_nH +
  scale_y_continuous(limits = c(10,100), trans = "log10")

tM_nE_log <- tM_nE +
  scale_y_continuous(limits = c(10,100), trans = "log10")

tH_nM_log <- tH_nM +
  scale_y_continuous(limits = c(10,100), trans = "log10")

tH_nE_log <- tH_nE +
  scale_y_continuous(limits = c(10,100), trans = "log10")


# output plots with patchwork

# target high null low
tE_nH_linear +
  ggtitle("Linear Scale") +
  facet_grid(~ Target + Null, labeller = labeller(Target = label_both, Null = label_both)) +

# target low null high
tH_nE_linear +
  facet_grid(~ Target + Null, labeller = labeller(Target = label_both, Null = label_both)) +

# target medium null low
tM_nH_linear +
  facet_grid(~ Target + Null, labeller = labeller(Target = label_both, Null = label_both)) +

# target low null medium
tH_nM_linear +
  facet_grid(~ Target + Null, labeller = labeller(Target = label_both, Null = label_both)) +
  
# target high null medium
tE_nM_linear +
  facet_grid(~ Target + Null, labeller = labeller(Target = label_both, Null = label_both)) +
  
# target medium null high
tM_nE_linear +
  facet_grid(~ Target + Null, labeller = labeller(Target = label_both, Null = label_both)) +


  
  
# target high null low
tE_nH_log +
  ggtitle("Log Scale") +
  facet_grid(~ Target + Null, labeller = labeller(Target = label_both, Null = label_both)) +

# target low null high
tH_nE_log +
  facet_grid(~ Target + Null, labeller = labeller(Target = label_both, Null = label_both)) +
  
# target medium null low
tM_nH_log +
  facet_grid(~ Target + Null, labeller = labeller(Target = label_both, Null = label_both)) +

# target low null medium
tH_nM_log +
  facet_grid(~ Target + Null, labeller = labeller(Target = label_both, Null = label_both)) +
  
# target high null medium
tE_nM_log +
  facet_grid(~ Target + Null, labeller = labeller(Target = label_both, Null = label_both)) +

  # target medium null high
tM_nE_log +
  facet_grid(~ Target + Null, labeller = labeller(Target = label_both, Null = label_both)) +
  
plot_layout(ncol = 6) 

# save thumbnails for plotting later

# tM_nH_thumbnail <- ((tM_nH_linear + ggtitle("Linear") + theme(plot.title = element_text(size = 70))) + (tM_nH_log+ ggtitle("Log"))) + theme(plot.title = element_text(size = 70))
# ggsave(tM_nH_thumbnail, file = "images/tM_nH_thumbnail-title.png", width = 6, height = 4)
# 
# tE_nH_thumbnail <- tE_nH_linear + tE_nH_log
# ggsave(tE_nH_thumbnail, file = "images/tE_nH_thumbnail.png", width = 6, height = 3)
# 
# tH_nM_thumbnail <- tH_nM_linear + tH_nM_log
# ggsave(tH_nM_thumbnail, file = "images/tH_nM_thumbnail.png", width = 6, height = 3)
# 
# tE_nM_thumbnail <- tE_nM_linear + tE_nM_log
# ggsave(tE_nM_thumbnail, file = "images/tE_nM_thumbnail.png", width = 6, height = 3)
# 
# tH_nE_thumbnail <- tH_nE_linear + tH_nE_log
# ggsave(tH_nE_thumbnail, file = "images/tH_nE_thumbnail.png", width = 6, height = 3)
# 
# tM_nE_thumbnail <- tM_nE_linear + tM_nE_log
# ggsave(tM_nE_thumbnail, file = "images/tM_nE_thumbnail.png", width = 6, height = 3)

# individual linear
# 
# ggsave(tM_nH_linear, file = "images/tM_nH_linear.png", width = 6, height = 6)
# ggsave(tE_nH_linear, file = "images/tE_nH_linear.png", width = 6, height = 6)
# ggsave(tH_nM_linear, file = "images/tH_nM_linear.png", width = 6, height = 6)
# ggsave(tE_nM_linear, file = "images/tE_nM_linear.png", width = 6, height = 6)
# ggsave(tH_nE_linear, file = "images/tH_nE_linear.png", width = 6, height = 6)
# ggsave(tM_nE_linear, file = "images/tM_nE_linear.png", width = 6, height = 6)
# 
# # individual log
# ggsave(tM_nH_log, file = "images/tM_nH_log.png", width = 6, height = 6)
# ggsave(tE_nH_log, file = "images/tE_nH_log.png", width = 6, height = 6)
# ggsave(tH_nM_log, file = "images/tH_nM_log.png", width = 6, height = 6)
# ggsave(tE_nM_log, file = "images/tE_nM_log.png", width = 6, height = 6)
# ggsave(tH_nE_log, file = "images/tH_nE_log.png", width = 6, height = 6)
# ggsave(tM_nE_log, file = "images/tM_nE_log.png", width = 6, height = 6)
```

\cref{fig:lineup-example} presents examples of statistical lineups with the target data simulated with exponential parameters corresponding high curvature and the surrounding null panels simulated with parameters for low curvature.
The statistical lineup on the left presents increasing exponential data with displayed on a linear scale with panel 13 as the target panel.
The lineup on the right shows increasing exponential data plotted on a log scale with panel 4 as the target panel.

```{r lineup-example, fig.height = 2.75, fig.width = 5.75, fig.cap = "The lineup plot on the left displays increasing exponential data on a linear scale with panel (13 as the target. The lineup plot on the right displays increasing exponential data on the log scale with panel 4 as the target."}

lineupData_linear <- read.csv(file = "data/lineup-example-data-linear.csv")

linearPlot <- ggplot(lineupData_linear, aes(x=x, y=y)) +
  facet_wrap(~.sample, ncol=5) +
  geom_point(size = .05) +
  theme(aspect.ratio = 1) +
  theme_bw(base_size = 14) +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        axis.text.y  = element_blank(),
        axis.text.x  = element_blank(),
        strip.text = element_text(size = 5, margin = margin(0.05,0,0.05,0, "cm")),
        strip.background = element_rect(linewidth = 0.5)
  )

lineupData_log <- read.csv(file = "data/lineup-example-data-log.csv")

logPlot <- ggplot(lineupData_log, aes(x=x, y=y)) +
  facet_wrap(~.sample, ncol=5) +
  geom_point(size = .05) +
  theme(aspect.ratio = 1) +
  theme_bw(base_size = 14) +
  theme(axis.title.y = element_blank(),
        axis.title.x = element_blank(),
        axis.text.y  = element_blank(),
        axis.text.x  = element_blank(),
        strip.text = element_text(size = 5, margin = margin(0.05,0,0.05,0, "cm")),
        strip.background = element_rect(linewidth = 0.5)
  ) +
  scale_y_continuous(trans = "log10")

linearPlot + logPlot
```


